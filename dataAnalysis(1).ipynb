{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This script is dedicated to the data analysis paper. in this script, csv files are being read and data is being extracted to look for further information about the data collected by three biosensor devices: Empatica device, mz3 watch and oura ring"
      ],
      "metadata": {
        "id": "X_Fc2-6R0zTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input all libraries needed"
      ],
      "metadata": {
        "id": "ZGEazN22wznz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from scipy.stats import ks_2samp\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "75Ikdmg1mQYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "read csv files"
      ],
      "metadata": {
        "id": "ik1rLTdZw4hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "oura_df = pd.read_csv('oura.csv', parse_dates=['timestamp'])\n",
        "mz3_df = pd.read_csv('mz3.csv', parse_dates=['Time'])\n",
        "empatica_df = pd.read_csv('empatica.csv', parse_dates=['minute'])"
      ],
      "metadata": {
        "id": "-yTxuaVJmZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set all times to one specfic unit so it will be easy for caluculations"
      ],
      "metadata": {
        "id": "blhioEf3w7aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns for consistency\n",
        "oura_df.rename(columns={'timestamp': 'Timestamp'}, inplace=True)\n",
        "mz3_df.rename(columns={'Time': 'Timestamp'}, inplace=True)\n",
        "empatica_df.rename(columns={'minute': 'Timestamp'}, inplace=True)\n",
        "\n",
        "# Remove timezone info if present\n",
        "oura_df['Timestamp'] = oura_df['Timestamp'].dt.tz_localize(None)\n",
        "mz3_df['Timestamp'] = mz3_df['Timestamp'].dt.tz_localize(None)\n",
        "empatica_df['Timestamp'] = empatica_df['Timestamp'].dt.tz_localize(None)"
      ],
      "metadata": {
        "id": "6VzhDk3EmxXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Timestamp as index and resample to 3-minute intervals\n",
        "oura_df.set_index('Timestamp', inplace=True)\n",
        "mz3_df.set_index('Timestamp', inplace=True)\n",
        "empatica_df.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Aggregate statistics\n",
        "oura_3min = oura_df.resample('3min').agg({'bpm': ['mean', 'max', 'min']})\n",
        "mz3_3min = mz3_df.resample('3min').agg({'hr': ['mean', 'max', 'min']})\n",
        "empatica_3min = empatica_df.resample('3min').agg({'entry_count': ['mean', 'max', 'min']})\n",
        "\n",
        "# Flatten column names\n",
        "oura_3min.columns = [f'bpm_{stat}' for stat in ['mean', 'max', 'min']]\n",
        "mz3_3min.columns = [f'hr_{stat}' for stat in ['mean', 'max', 'min']]\n",
        "empatica_3min.columns = [f'entry_count_{stat}' for stat in ['mean', 'max', 'min']]\n"
      ],
      "metadata": {
        "id": "zq7WYXvpm2ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get summary stats for a specific time interval or ativity. summary stats will be calculated every 3 mins over a 30 min window"
      ],
      "metadata": {
        "id": "N14mONLQxGfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day 2: 1/22 rest activity"
      ],
      "metadata": {
        "id": "dmZ1f8GtxqOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific time interval\n",
        "\n",
        "start_time = '2024-01-22 05:36:00'\n",
        "end_time = '2024-01-22 06:06:00'\n",
        "\n",
        "# Resample the data to 3-minute intervals and compute more statistics\n",
        "oura_3min_stats = oura_df.resample('3min').agg({\n",
        "    'bpm': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "mz3_3min_stats = mz3_df.resample('3min').agg({\n",
        "    'hr': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "empatica_3min_stats = empatica_df.resample('3min').agg({\n",
        "    'entry_count': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "# Flatten the multi-level column names for better readability\n",
        "oura_3min_stats.columns = [f'bpm_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "mz3_3min_stats.columns = [f'hr_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "empatica_3min_stats.columns = [f'entry_count_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "# Filter the data for the specified time interval\n",
        "filtered_oura = oura_3min_stats.loc[start_time:end_time]\n",
        "filtered_mz3 = mz3_3min_stats.loc[start_time:end_time]\n",
        "filtered_empatica = empatica_3min_stats.loc[start_time:end_time]\n",
        "\n",
        "# Combine the statistics into one summary DataFrame\n",
        "#summary_stats = pd.concat([filtered_oura, filtered_mz3, filtered_empatica], axis=1)\n",
        "\n",
        "# Print the summary statistics for each 3-minute interval\n",
        "#print(summary_stats)\n",
        "\n",
        "# Print the Oura statistics\n",
        "print(\"Oura Statistics\")\n",
        "print(filtered_oura)\n",
        "\n",
        "# Print the MZ3 statistics\n",
        "print(\"\\nMZ3 Statistics\")\n",
        "print(filtered_mz3)\n",
        "\n",
        "# Print the Empatica statistics\n",
        "print(\"\\nEmpatica Statistics\")\n",
        "print(filtered_empatica)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WesqXguownPE",
        "outputId": "4d575ec2-336b-414b-9443-9f5eb91b9ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oura Statistics\n",
            "                      bpm_mean  bpm_max  bpm_min   bpm_std  bpm_count\n",
            "Timestamp                                                            \n",
            "2024-01-22 05:36:00  63.000000     64.0     62.0  1.414214          2\n",
            "2024-01-22 05:39:00  66.500000     67.0     66.0  0.707107          2\n",
            "2024-01-22 05:42:00  69.000000     69.0     69.0       NaN          1\n",
            "2024-01-22 05:45:00  69.333333     72.0     67.0  2.516611          3\n",
            "2024-01-22 05:48:00        NaN      NaN      NaN       NaN          0\n",
            "2024-01-22 05:51:00  68.000000     71.0     65.0  3.000000          3\n",
            "2024-01-22 05:54:00  70.000000     70.0     70.0       NaN          1\n",
            "2024-01-22 05:57:00  72.500000     74.0     71.0  2.121320          2\n",
            "2024-01-22 06:00:00  65.000000     66.0     64.0  1.000000          3\n",
            "2024-01-22 06:03:00        NaN      NaN      NaN       NaN          0\n",
            "2024-01-22 06:06:00  65.666667     67.0     64.0  1.527525          3\n",
            "\n",
            "MZ3 Statistics\n",
            "                       hr_mean  hr_max  hr_min    hr_std  hr_count\n",
            "Timestamp                                                         \n",
            "2024-01-22 05:36:00  82.666667    86.0    81.0  2.886751         3\n",
            "2024-01-22 05:39:00  85.000000    86.0    84.0  1.000000         3\n",
            "2024-01-22 05:42:00  84.000000    85.0    83.0  1.000000         3\n",
            "2024-01-22 05:45:00  86.000000    92.0    81.0  5.567764         3\n",
            "2024-01-22 05:48:00  82.000000    86.0    80.0  3.464102         3\n",
            "2024-01-22 05:51:00  79.000000    81.0    77.0  2.000000         3\n",
            "2024-01-22 05:54:00  72.333333    77.0    68.0  4.509250         3\n",
            "2024-01-22 05:57:00  77.666667    80.0    76.0  2.081666         3\n",
            "2024-01-22 06:00:00  65.666667    66.0    65.0  0.577350         3\n",
            "2024-01-22 06:03:00  72.666667    80.0    64.0  8.082904         3\n",
            "2024-01-22 06:06:00  69.333333    74.0    65.0  4.509250         3\n",
            "\n",
            "Empatica Statistics\n",
            "                     entry_count_mean  entry_count_max  entry_count_min  \\\n",
            "Timestamp                                                                 \n",
            "2024-01-22 05:36:00         66.666667             68.0             65.0   \n",
            "2024-01-22 05:39:00         65.333333             67.0             63.0   \n",
            "2024-01-22 05:42:00         66.666667             70.0             64.0   \n",
            "2024-01-22 05:45:00         68.333333             71.0             66.0   \n",
            "2024-01-22 05:48:00         65.333333             66.0             65.0   \n",
            "2024-01-22 05:51:00         67.333333             70.0             66.0   \n",
            "2024-01-22 05:54:00         68.666667             71.0             65.0   \n",
            "2024-01-22 05:57:00         68.000000             72.0             66.0   \n",
            "2024-01-22 06:00:00         66.666667             70.0             64.0   \n",
            "2024-01-22 06:03:00         64.333333             67.0             62.0   \n",
            "2024-01-22 06:06:00         66.000000             66.0             66.0   \n",
            "\n",
            "                     entry_count_std  entry_count_count  \n",
            "Timestamp                                                \n",
            "2024-01-22 05:36:00         1.527525                  3  \n",
            "2024-01-22 05:39:00         2.081666                  3  \n",
            "2024-01-22 05:42:00         3.055050                  3  \n",
            "2024-01-22 05:45:00         2.516611                  3  \n",
            "2024-01-22 05:48:00         0.577350                  3  \n",
            "2024-01-22 05:51:00         2.309401                  3  \n",
            "2024-01-22 05:54:00         3.214550                  3  \n",
            "2024-01-22 05:57:00         3.464102                  3  \n",
            "2024-01-22 06:00:00         3.055050                  3  \n",
            "2024-01-22 06:03:00         2.516611                  3  \n",
            "2024-01-22 06:06:00         0.000000                  3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day 2: 1/22 house work activity"
      ],
      "metadata": {
        "id": "S7QTtZlIxvsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = \"2024-01-22T07:34:00\"\n",
        "end_time = \"2024-01-22T08:04:00\"\n",
        "# Define the specific time interval\n",
        "\n",
        "# Resample the data to 3-minute intervals and compute more statistics\n",
        "oura_3min_stats = oura_df.resample('3min').agg({\n",
        "    'bpm': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "mz3_3min_stats = mz3_df.resample('3min').agg({\n",
        "    'hr': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "empatica_3min_stats = empatica_df.resample('3min').agg({\n",
        "    'entry_count': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "# Flatten the multi-level column names for better readability\n",
        "oura_3min_stats.columns = [f'bpm_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "mz3_3min_stats.columns = [f'hr_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "empatica_3min_stats.columns = [f'entry_count_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "# Filter the data for the specified time interval\n",
        "filtered_oura = oura_3min_stats.loc[start_time:end_time]\n",
        "filtered_mz3 = mz3_3min_stats.loc[start_time:end_time]\n",
        "filtered_empatica = empatica_3min_stats.loc[start_time:end_time]\n",
        "\n",
        "# Combine the statistics into one summary DataFrame\n",
        "summary_stats = pd.concat([filtered_oura, filtered_mz3, filtered_empatica], axis=1)\n",
        "\n",
        "# Print the summary statistics for each 3-minute interval\n",
        "print(summary_stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2WrU9rGx0SU",
        "outputId": "c8daf1b1-ff7a-4f49-f51b-2140825fd655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     bpm_mean  bpm_max  bpm_min  bpm_std  bpm_count  \\\n",
            "Timestamp                                                             \n",
            "2024-01-22 07:36:00      61.0     61.0     61.0      NaN          1   \n",
            "2024-01-22 07:39:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 07:42:00      61.0     61.0     61.0      NaN          1   \n",
            "2024-01-22 07:45:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 07:48:00      62.0     62.0     62.0      NaN          1   \n",
            "2024-01-22 07:51:00      65.0     65.0     65.0      NaN          1   \n",
            "2024-01-22 07:54:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 07:57:00      66.0     66.0     66.0      NaN          1   \n",
            "2024-01-22 08:00:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 08:03:00      64.0     64.0     64.0      NaN          1   \n",
            "\n",
            "                       hr_mean  hr_max  hr_min     hr_std  hr_count  \\\n",
            "Timestamp                                                             \n",
            "2024-01-22 07:36:00  75.666667    78.0    74.0   2.081666         3   \n",
            "2024-01-22 07:39:00  78.000000    79.0    77.0   1.000000         3   \n",
            "2024-01-22 07:42:00  79.333333    80.0    78.0   1.154701         3   \n",
            "2024-01-22 07:45:00  76.333333    77.0    76.0   0.577350         3   \n",
            "2024-01-22 07:48:00  76.333333    77.0    76.0   0.577350         3   \n",
            "2024-01-22 07:51:00  76.333333    78.0    75.0   1.527525         3   \n",
            "2024-01-22 07:54:00  75.666667    77.0    74.0   1.527525         3   \n",
            "2024-01-22 07:57:00  77.000000    79.0    75.0   2.000000         3   \n",
            "2024-01-22 08:00:00  77.666667    79.0    77.0   1.154701         3   \n",
            "2024-01-22 08:03:00  84.000000    90.0    72.0  10.392305         3   \n",
            "\n",
            "                     entry_count_mean  entry_count_max  entry_count_min  \\\n",
            "Timestamp                                                                 \n",
            "2024-01-22 07:36:00         64.333333             66.0             61.0   \n",
            "2024-01-22 07:39:00         61.666667             63.0             60.0   \n",
            "2024-01-22 07:42:00         61.333333             63.0             60.0   \n",
            "2024-01-22 07:45:00         62.333333             65.0             60.0   \n",
            "2024-01-22 07:48:00         61.333333             62.0             60.0   \n",
            "2024-01-22 07:51:00         63.000000             65.0             61.0   \n",
            "2024-01-22 07:54:00         63.000000             65.0             62.0   \n",
            "2024-01-22 07:57:00         67.666667             73.0             63.0   \n",
            "2024-01-22 08:00:00         66.333333             68.0             64.0   \n",
            "2024-01-22 08:03:00         65.000000             67.0             63.0   \n",
            "\n",
            "                     entry_count_std  entry_count_count  \n",
            "Timestamp                                                \n",
            "2024-01-22 07:36:00         2.886751                  3  \n",
            "2024-01-22 07:39:00         1.527525                  3  \n",
            "2024-01-22 07:42:00         1.527525                  3  \n",
            "2024-01-22 07:45:00         2.516611                  3  \n",
            "2024-01-22 07:48:00         1.154701                  3  \n",
            "2024-01-22 07:51:00         2.000000                  3  \n",
            "2024-01-22 07:54:00         1.732051                  3  \n",
            "2024-01-22 07:57:00         5.033223                  3  \n",
            "2024-01-22 08:00:00         2.081666                  3  \n",
            "2024-01-22 08:03:00         2.000000                  3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day 2: 1/22 computer work\n"
      ],
      "metadata": {
        "id": "gKBWY0YnyDae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = \"2024-01-22T08:07:00\"\n",
        "end_time = \"2024-01-22T08:37:00\"\n",
        "# Define the specific time interval\n",
        "\n",
        "# Resample the data to 3-minute intervals and compute more statistics\n",
        "oura_3min_stats = oura_df.resample('3min').agg({\n",
        "    'bpm': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "mz3_3min_stats = mz3_df.resample('3min').agg({\n",
        "    'hr': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "empatica_3min_stats = empatica_df.resample('3min').agg({\n",
        "    'entry_count': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "# Flatten the multi-level column names for better readability\n",
        "oura_3min_stats.columns = [f'bpm_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "mz3_3min_stats.columns = [f'hr_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "empatica_3min_stats.columns = [f'entry_count_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "# Filter the data for the specified time interval\n",
        "filtered_oura = oura_3min_stats.loc[start_time:end_time]\n",
        "filtered_mz3 = mz3_3min_stats.loc[start_time:end_time]\n",
        "filtered_empatica = empatica_3min_stats.loc[start_time:end_time]\n",
        "\n",
        "# Combine the statistics into one summary DataFrame\n",
        "summary_stats = pd.concat([filtered_oura, filtered_mz3, filtered_empatica], axis=1)\n",
        "\n",
        "# Print the summary statistics for each 3-minute interval\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wU7MlGygWd",
        "outputId": "bc6e35c8-cf69-45a1-9d63-1ab5f2f4a721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     bpm_mean  bpm_max  bpm_min  bpm_std  bpm_count  \\\n",
            "Timestamp                                                             \n",
            "2024-01-22 08:09:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 08:12:00      62.0     62.0     62.0      NaN          1   \n",
            "2024-01-22 08:15:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 08:18:00      63.0     63.0     63.0      NaN          1   \n",
            "2024-01-22 08:21:00      63.0     63.0     63.0      NaN          1   \n",
            "2024-01-22 08:24:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 08:27:00      62.0     62.0     62.0      NaN          1   \n",
            "2024-01-22 08:30:00       NaN      NaN      NaN      NaN          0   \n",
            "2024-01-22 08:33:00      63.0     63.0     63.0      NaN          1   \n",
            "2024-01-22 08:36:00      63.0     63.0     63.0      NaN          1   \n",
            "\n",
            "                       hr_mean  hr_max  hr_min    hr_std  hr_count  \\\n",
            "Timestamp                                                            \n",
            "2024-01-22 08:09:00  72.000000    74.0    69.0  2.645751         3   \n",
            "2024-01-22 08:12:00  75.333333    79.0    71.0  4.041452         3   \n",
            "2024-01-22 08:15:00  71.666667    72.0    71.0  0.577350         3   \n",
            "2024-01-22 08:18:00  74.333333    75.0    74.0  0.577350         3   \n",
            "2024-01-22 08:21:00  71.666667    73.0    71.0  1.154701         3   \n",
            "2024-01-22 08:24:00  75.000000    77.0    74.0  1.732051         3   \n",
            "2024-01-22 08:27:00  77.333333    78.0    77.0  0.577350         3   \n",
            "2024-01-22 08:30:00  77.666667    79.0    75.0  2.309401         3   \n",
            "2024-01-22 08:33:00  71.666667    72.0    71.0  0.577350         3   \n",
            "2024-01-22 08:36:00  72.666667    74.0    71.0  1.527525         3   \n",
            "\n",
            "                     entry_count_mean  entry_count_max  entry_count_min  \\\n",
            "Timestamp                                                                 \n",
            "2024-01-22 08:09:00         62.333333             63.0             62.0   \n",
            "2024-01-22 08:12:00         62.000000             62.0             62.0   \n",
            "2024-01-22 08:15:00         62.666667             63.0             62.0   \n",
            "2024-01-22 08:18:00         62.333333             63.0             62.0   \n",
            "2024-01-22 08:21:00         63.000000             64.0             62.0   \n",
            "2024-01-22 08:24:00         63.000000             63.0             63.0   \n",
            "2024-01-22 08:27:00         63.000000             63.0             63.0   \n",
            "2024-01-22 08:30:00         62.333333             63.0             62.0   \n",
            "2024-01-22 08:33:00         63.000000             63.0             63.0   \n",
            "2024-01-22 08:36:00         63.666667             64.0             63.0   \n",
            "\n",
            "                     entry_count_std  entry_count_count  \n",
            "Timestamp                                                \n",
            "2024-01-22 08:09:00          0.57735                  3  \n",
            "2024-01-22 08:12:00          0.00000                  3  \n",
            "2024-01-22 08:15:00          0.57735                  3  \n",
            "2024-01-22 08:18:00          0.57735                  3  \n",
            "2024-01-22 08:21:00          1.00000                  3  \n",
            "2024-01-22 08:24:00          0.00000                  3  \n",
            "2024-01-22 08:27:00          0.00000                  3  \n",
            "2024-01-22 08:30:00          0.57735                  3  \n",
            "2024-01-22 08:33:00          0.00000                  3  \n",
            "2024-01-22 08:36:00          0.57735                  3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day 2: 1/22 exercise activity"
      ],
      "metadata": {
        "id": "7T-9Ag2szN79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = \"2024-01-22T06:14:00\"\n",
        "end_time = \"2024-01-22T06:44:00\"\n",
        "# Define the specific time interval\n",
        "\n",
        "# Resample the data to 3-minute intervals and compute more statistics\n",
        "oura_3min_stats = oura_df.resample('3min').agg({\n",
        "    'bpm': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "mz3_3min_stats = mz3_df.resample('3min').agg({\n",
        "    'hr': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "empatica_3min_stats = empatica_df.resample('3min').agg({\n",
        "    'entry_count': ['mean', 'max', 'min', 'std', 'count']  # Adding std and count\n",
        "})\n",
        "\n",
        "# Flatten the multi-level column names for better readability\n",
        "oura_3min_stats.columns = [f'bpm_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "mz3_3min_stats.columns = [f'hr_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "empatica_3min_stats.columns = [f'entry_count_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "# Filter the data for the specified time interval\n",
        "filtered_oura = oura_3min_stats.loc[start_time:end_time]\n",
        "filtered_mz3 = mz3_3min_stats.loc[start_time:end_time]\n",
        "filtered_empatica = empatica_3min_stats.loc[start_time:end_time]\n",
        "\n",
        "# Combine the statistics into one summary DataFrame\n",
        "summary_stats = pd.concat([filtered_oura, filtered_mz3, filtered_empatica], axis=1)\n",
        "\n",
        "# Print the summary statistics for each 3-minute interval\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc0rVXEJyzAo",
        "outputId": "bb4d8306-1991-4a69-e1b9-236c666bf501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      bpm_mean  bpm_max  bpm_min   bpm_std  bpm_count  \\\n",
            "Timestamp                                                               \n",
            "2024-01-22 06:15:00  66.500000     67.0     66.0  0.707107          2   \n",
            "2024-01-22 06:18:00  64.000000     64.0     64.0       NaN          1   \n",
            "2024-01-22 06:21:00  65.000000     67.0     63.0  2.000000          3   \n",
            "2024-01-22 06:24:00        NaN      NaN      NaN       NaN          0   \n",
            "2024-01-22 06:27:00  66.333333     68.0     65.0  1.527525          3   \n",
            "2024-01-22 06:30:00  66.000000     67.0     65.0  1.414214          2   \n",
            "2024-01-22 06:33:00  63.000000     63.0     63.0       NaN          1   \n",
            "2024-01-22 06:36:00  65.666667     68.0     64.0  2.081666          3   \n",
            "2024-01-22 06:39:00        NaN      NaN      NaN       NaN          0   \n",
            "2024-01-22 06:42:00  66.000000     67.0     65.0  1.000000          3   \n",
            "\n",
            "                        hr_mean  hr_max  hr_min    hr_std  hr_count  \\\n",
            "Timestamp                                                             \n",
            "2024-01-22 06:15:00  106.666667   109.0   104.0  2.516611         3   \n",
            "2024-01-22 06:18:00  108.000000   110.0   106.0  2.000000         3   \n",
            "2024-01-22 06:21:00  108.000000   109.0   107.0  1.000000         3   \n",
            "2024-01-22 06:24:00  106.666667   107.0   106.0  0.577350         3   \n",
            "2024-01-22 06:27:00  104.333333   106.0   102.0  2.081666         3   \n",
            "2024-01-22 06:30:00  106.333333   108.0   104.0  2.081666         3   \n",
            "2024-01-22 06:33:00  105.000000   106.0   103.0  1.732051         3   \n",
            "2024-01-22 06:36:00  106.000000   109.0   104.0  2.645751         3   \n",
            "2024-01-22 06:39:00  108.000000   113.0   103.0  5.000000         3   \n",
            "2024-01-22 06:42:00  111.666667   115.0   106.0  4.932883         3   \n",
            "\n",
            "                     entry_count_mean  entry_count_max  entry_count_min  \\\n",
            "Timestamp                                                                 \n",
            "2024-01-22 06:15:00         64.666667             66.0             63.0   \n",
            "2024-01-22 06:18:00         66.333333             68.0             64.0   \n",
            "2024-01-22 06:21:00         65.333333             66.0             65.0   \n",
            "2024-01-22 06:24:00         67.333333             69.0             66.0   \n",
            "2024-01-22 06:27:00         66.000000             68.0             64.0   \n",
            "2024-01-22 06:30:00         65.666667             67.0             63.0   \n",
            "2024-01-22 06:33:00         66.000000             67.0             64.0   \n",
            "2024-01-22 06:36:00         66.666667             68.0             65.0   \n",
            "2024-01-22 06:39:00         65.666667             66.0             65.0   \n",
            "2024-01-22 06:42:00         66.666667             68.0             66.0   \n",
            "\n",
            "                     entry_count_std  entry_count_count  \n",
            "Timestamp                                                \n",
            "2024-01-22 06:15:00         1.527525                  3  \n",
            "2024-01-22 06:18:00         2.081666                  3  \n",
            "2024-01-22 06:21:00         0.577350                  3  \n",
            "2024-01-22 06:24:00         1.527525                  3  \n",
            "2024-01-22 06:27:00         2.000000                  3  \n",
            "2024-01-22 06:30:00         2.309401                  3  \n",
            "2024-01-22 06:33:00         1.732051                  3  \n",
            "2024-01-22 06:36:00         1.527525                  3  \n",
            "2024-01-22 06:39:00         0.577350                  3  \n",
            "2024-01-22 06:42:00         1.154701                  3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the datasets\n",
        "merged_df = pd.merge(oura_3min, mz3_3min, left_index=True, right_index=True, suffixes=('_oura', '_mz3'))\n",
        "merged_df = pd.merge(merged_df, empatica_3min, left_index=True, right_index=True)\n",
        "\n",
        "# Define intervals for testing\n",
        "intervals = [\n",
        "    (\"2024-01-22T05:36:00\", \"2024-01-22T06:06:00\"), (\"2024-01-22T06:14:00\", \"2024-01-22T06:44:00\"),\n",
        "    (\"2024-01-22T07:34:00\", \"2024-01-22T08:04:00\"), (\"2024-01-22T06:14:00\", \"2024-01-22T06:44:00\"  )\n",
        "    # Add more intervals as needed\n",
        "]"
      ],
      "metadata": {
        "id": "JaG7h5n8m9kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate statistical functions: ks test, correlation and p -value\n",
        "- for every 30 min window, script prints these stats for oura, mz3 and empatica"
      ],
      "metadata": {
        "id": "rHFMSohMzj1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical functions\n",
        "def ks_test(series1, series2):\n",
        "    stat, p_value = ks_2samp(series1.dropna(), series2.dropna())\n",
        "    return stat, p_value\n",
        "\n",
        "def correlation(series1, series2):\n",
        "    return series1.corr(series2)\n",
        "\n",
        "# Iterate through intervals\n",
        "for start_time, end_time in intervals:\n",
        "    print(f\"\\nInterval: {start_time} to {end_time}\")\n",
        "    filtered_df = merged_df.loc[start_time:end_time]\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        print(\"No data available for this interval.\")\n",
        "        continue\n",
        "\n",
        "    # Perform KS Test and Correlation for each stat ('mean', 'max', 'min')\n",
        "    for stat in ['mean', 'max', 'min']:\n",
        "        col_oura = f'bpm_{stat}'\n",
        "        col_mz3 = f'hr_{stat}'\n",
        "        col_empatica = f'entry_count_{stat}'\n",
        "\n",
        "        # Skip if columns are missing\n",
        "        if not all(col in filtered_df.columns for col in [col_oura, col_mz3, col_empatica]):\n",
        "            print(f\"Missing columns for {stat}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        ks_oura_mz3 = ks_test(filtered_df[col_oura], filtered_df[col_mz3])\n",
        "        ks_oura_empatica = ks_test(filtered_df[col_oura], filtered_df[col_empatica])\n",
        "        ks_mz3_empatica = ks_test(filtered_df[col_mz3], filtered_df[col_empatica])\n",
        "\n",
        "        corr_oura_mz3 = correlation(filtered_df[col_oura], filtered_df[col_mz3])\n",
        "        corr_oura_empatica = correlation(filtered_df[col_oura], filtered_df[col_empatica])\n",
        "        corr_mz3_empatica = correlation(filtered_df[col_mz3], filtered_df[col_empatica])\n",
        "\n",
        "        print(f\"\\nStatistic Results for {stat.capitalize()}:\")\n",
        "        print(f\"KS Test (Oura vs MZ3): Statistic={ks_oura_mz3[0]:.4f}, p-value={ks_oura_mz3[1]:.4f}\")\n",
        "        print(f\"KS Test (Oura vs Empatica): Statistic={ks_oura_empatica[0]:.4f}, p-value={ks_oura_empatica[1]:.4f}\")\n",
        "        print(f\"KS Test (MZ3 vs Empatica): Statistic={ks_mz3_empatica[0]:.4f}, p-value={ks_mz3_empatica[1]:.4f}\")\n",
        "        print(f\"Correlation (Oura vs MZ3): {corr_oura_mz3:.4f}\")\n",
        "        print(f\"Correlation (Oura vs Empatica): {corr_oura_empatica:.4f}\")\n",
        "        print(f\"Correlation (MZ3 vs Empatica): {corr_mz3_empatica:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INxANMB4nA9c",
        "outputId": "eb565a80-2386-462f-ae49-ab0026c59211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Interval: 2024-01-22T05:36:00 to 2024-01-22T06:06:00\n",
            "\n",
            "Statistic Results for Mean:\n",
            "KS Test (Oura vs MZ3): Statistic=0.7273, p-value=0.0041\n",
            "KS Test (Oura vs Empatica): Statistic=0.4444, p-value=0.2083\n",
            "KS Test (MZ3 vs Empatica): Statistic=0.9091, p-value=0.0001\n",
            "Correlation (Oura vs MZ3): 0.1586\n",
            "Correlation (Oura vs Empatica): 0.6623\n",
            "Correlation (MZ3 vs Empatica): 0.0617\n",
            "\n",
            "Statistic Results for Max:\n",
            "KS Test (Oura vs MZ3): Statistic=0.8182, p-value=0.0008\n",
            "KS Test (Oura vs Empatica): Statistic=0.1313, p-value=0.9998\n",
            "KS Test (MZ3 vs Empatica): Statistic=0.9091, p-value=0.0001\n",
            "Correlation (Oura vs MZ3): 0.2431\n",
            "Correlation (Oura vs Empatica): 0.7334\n",
            "Correlation (MZ3 vs Empatica): -0.0444\n",
            "\n",
            "Statistic Results for Min:\n",
            "KS Test (Oura vs MZ3): Statistic=0.6364, p-value=0.0187\n",
            "KS Test (Oura vs Empatica): Statistic=0.4444, p-value=0.2083\n",
            "KS Test (MZ3 vs Empatica): Statistic=0.7273, p-value=0.0044\n",
            "Correlation (Oura vs MZ3): 0.1007\n",
            "Correlation (Oura vs Empatica): 0.0734\n",
            "Correlation (MZ3 vs Empatica): 0.1485\n",
            "\n",
            "Interval: 2024-01-22T06:14:00 to 2024-01-22T06:44:00\n",
            "\n",
            "Statistic Results for Mean:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.3000, p-value=0.7227\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.1154\n",
            "Correlation (Oura vs Empatica): -0.2276\n",
            "Correlation (MZ3 vs Empatica): 0.1415\n",
            "\n",
            "Statistic Results for Max:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.2500, p-value=0.8982\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.1641\n",
            "Correlation (Oura vs Empatica): 0.0218\n",
            "Correlation (MZ3 vs Empatica): -0.1665\n",
            "\n",
            "Statistic Results for Min:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.0750, p-value=1.0000\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): -0.2789\n",
            "Correlation (Oura vs Empatica): -0.3578\n",
            "Correlation (MZ3 vs Empatica): 0.4676\n",
            "\n",
            "Interval: 2024-01-22T07:34:00 to 2024-01-22T08:04:00\n",
            "\n",
            "Statistic Results for Mean:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0002\n",
            "KS Test (Oura vs Empatica): Statistic=0.3333, p-value=0.7125\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.0956\n",
            "Correlation (Oura vs Empatica): 0.6766\n",
            "Correlation (MZ3 vs Empatica): 0.1415\n",
            "\n",
            "Statistic Results for Max:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0002\n",
            "KS Test (Oura vs Empatica): Statistic=0.4000, p-value=0.5055\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.1868\n",
            "Correlation (Oura vs Empatica): 0.7202\n",
            "Correlation (MZ3 vs Empatica): 0.2017\n",
            "\n",
            "Statistic Results for Min:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0002\n",
            "KS Test (Oura vs Empatica): Statistic=0.4000, p-value=0.5055\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): -0.3744\n",
            "Correlation (Oura vs Empatica): 0.7307\n",
            "Correlation (MZ3 vs Empatica): -0.3988\n",
            "\n",
            "Interval: 2024-01-22T06:14:00 to 2024-01-22T06:44:00\n",
            "\n",
            "Statistic Results for Mean:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.3000, p-value=0.7227\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.1154\n",
            "Correlation (Oura vs Empatica): -0.2276\n",
            "Correlation (MZ3 vs Empatica): 0.1415\n",
            "\n",
            "Statistic Results for Max:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.2500, p-value=0.8982\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): 0.1641\n",
            "Correlation (Oura vs Empatica): 0.0218\n",
            "Correlation (MZ3 vs Empatica): -0.1665\n",
            "\n",
            "Statistic Results for Min:\n",
            "KS Test (Oura vs MZ3): Statistic=1.0000, p-value=0.0000\n",
            "KS Test (Oura vs Empatica): Statistic=0.0750, p-value=1.0000\n",
            "KS Test (MZ3 vs Empatica): Statistic=1.0000, p-value=0.0000\n",
            "Correlation (Oura vs MZ3): -0.2789\n",
            "Correlation (Oura vs Empatica): -0.3578\n",
            "Correlation (MZ3 vs Empatica): 0.4676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD ANALYSIS"
      ],
      "metadata": {
        "id": "nqzTctLqf43P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extract_device_data(device_name, start_time, end_time, output_file):\n",
        "    # Load datasets (files that we have)\n",
        "    oura_df = pd.read_csv('oura.csv', parse_dates=['timestamp'])\n",
        "    mz3_df = pd.read_csv('mz3.csv', parse_dates=['Time'])\n",
        "    empatica_df = pd.read_csv('empatica.csv', parse_dates=['minute'])\n",
        "\n",
        "    # Standardize column names so that when we Filter data, it is easier to call columns 'timestamp'\n",
        "    oura_df.rename(columns={'timestamp': 'Timestamp'}, inplace=True)\n",
        "    mz3_df.rename(columns={'Time': 'Timestamp'}, inplace=True)\n",
        "    empatica_df.rename(columns={'minute': 'Timestamp'}, inplace=True)\n",
        "\n",
        "    # Remove timezone info if present\n",
        "    #for df in [oura_df, mz3_df, empatica_df]:\n",
        "        #df['Timestamp'] = df['Timestamp'].dt.tz_localize(None)\n",
        "\n",
        "    # Set Timestamp as index\n",
        "    oura_df.set_index('Timestamp', inplace=True)\n",
        "    mz3_df.set_index('Timestamp', inplace=True)\n",
        "    empatica_df.set_index('Timestamp', inplace=True)\n",
        "\n",
        "    # Dictionary mapping device names to their corresponding data and column name\n",
        "    devices = {\n",
        "        'oura': (oura_df, 'bpm'),\n",
        "        'mz3': (mz3_df, 'hr'),\n",
        "        'empatica': (empatica_df, 'entry_count')\n",
        "    }\n",
        "\n",
        "    if device_name not in devices:\n",
        "        print(\"Invalid device name. Choose from: oura, mz3, empatica\")\n",
        "        return\n",
        "\n",
        "    df, col_name = devices[device_name]\n",
        "\n",
        "    # Resample and compute statistics\n",
        "    df_resampled = df.resample('3min').agg({col_name: ['mean', 'max', 'min', 'std']})\n",
        "    df_resampled.columns = ['mean', 'max', 'min', 'std']\n",
        "\n",
        "    # Filter by the specified time range\n",
        "    filtered_data = df_resampled.loc[start_time:end_time].reset_index()\n",
        "    filtered_data['date'] = filtered_data['Timestamp'].dt.date\n",
        "\n",
        "    # Save to CSV\n",
        "    filtered_data.to_csv(output_file, index=False)\n",
        "    print(f\"Extracted data saved to {output_file}\")\n",
        "\n",
        "# Example usage in Google Colab:\n",
        "device = 'oura'  # Change to 'mz3' or 'empatica' as needed\n",
        "start_time = '2024-01-22 05:36:00'\n",
        "end_time = '2024-01-22 06:06:00'\n",
        "output_file = 'output.csv'\n",
        "\n",
        "extract_device_data(device, start_time, end_time, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTkTaGyTgfBv",
        "outputId": "095f16f1-408a-4c5e-c208-adc2b128df37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted data saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW ANALYSIS\n"
      ],
      "metadata": {
        "id": "C2GxcnEqf8l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_device_data(device_name, time_ranges, output_file):\n",
        "    # Load datasets\n",
        "    oura_df = pd.read_csv('oura.csv', parse_dates=['timestamp'])\n",
        "    mz3_df = pd.read_csv('mz3.csv', parse_dates=['Time'])\n",
        "    empatica_df = pd.read_csv('empatica.csv', parse_dates=['minute'])\n",
        "\n",
        "    # Standardize column names\n",
        "    oura_df.rename(columns={'timestamp': 'Timestamp'}, inplace=True)\n",
        "    mz3_df.rename(columns={'Time': 'Timestamp'}, inplace=True)\n",
        "    empatica_df.rename(columns={'minute': 'Timestamp'}, inplace=True)\n",
        "\n",
        "    # Remove timezone info if present\n",
        "    for df in [oura_df, mz3_df, empatica_df]:\n",
        "        df['Timestamp'] = df['Timestamp'].dt.tz_localize(None)\n",
        "\n",
        "\n",
        "    # Convert the timestamp to datetime and convert to Central Time (CT)\n",
        "    for df in [oura_df, mz3_df, empatica_df]:\n",
        "    # Convert to datetime (if it's not already)\n",
        "      df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "    # If the Timestamp is already timezone-aware, convert it to Central Time\n",
        "      if df['Timestamp'].dt.tz is not None:\n",
        "          df['Timestamp'] = df['Timestamp'].dt.tz_convert('US/Central')\n",
        "      else:\n",
        "        # If the Timestamp is naive (no timezone), localize it to UTC first, then convert\n",
        "          df['Timestamp'] = df['Timestamp'].dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
        "\n",
        "\n",
        "    # Retain original timezone instead of removing it\n",
        "    #for df in [oura_df, mz3_df, empatica_df]:\n",
        "      #df['Timestamp'] = pd.to_datetime(df['Timestamp'], ct=True)\n",
        "\n",
        "\n",
        "    # Set Timestamp as index\n",
        "    oura_df.set_index('Timestamp', inplace=True)\n",
        "    mz3_df.set_index('Timestamp', inplace=True)\n",
        "    empatica_df.set_index('Timestamp', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Dictionary mapping device names to their corresponding data and column name\n",
        "    devices = {\n",
        "        'oura': (oura_df, 'bpm'),\n",
        "        'mz3': (mz3_df, 'hr'),\n",
        "        'empatica': (empatica_df, 'entry_count')\n",
        "    }\n",
        "\n",
        "    if device_name not in devices:\n",
        "        print(\"Invalid device name. Choose from: oura, mz3, empatica\")\n",
        "        return\n",
        "\n",
        "    df, col_name = devices[device_name]\n",
        "\n",
        "    # Resample and compute statistics\n",
        "    df_resampled = df.resample('3min').agg({col_name: ['mean', 'max', 'min', 'std', 'count']})\n",
        "    df_resampled.columns = ['mean', 'max', 'min', 'std', 'count']\n",
        "\n",
        "    # Collect filtered data for multiple time ranges\n",
        "    results = []\n",
        "    for start_time, end_time in time_ranges:\n",
        "        filtered_data = df_resampled.loc[start_time:end_time].reset_index()\n",
        "        filtered_data['Date'] = filtered_data['Timestamp'].dt.strftime('%m/%d/%Y')\n",
        "        filtered_data['Time'] = filtered_data['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "        results.append(filtered_data)\n",
        "\n",
        "    # Concatenate all results\n",
        "    final_data = pd.concat(results, ignore_index=True)\n",
        "    final_data = final_data[['Date', 'Time', 'mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "    # Save to CSV\n",
        "    final_data.to_csv(output_file, index=False)\n",
        "    print(f\"Extracted data saved to {output_file}\")\n",
        "\n",
        "# Example usage in Google Colab:\n",
        "device = 'oura'  # Change to 'mz3' or 'empatica' as needed\n",
        "time_ranges = [\n",
        "\n",
        "    ('2024-01-22 04:00:00', '2024-01-22 04:30:00'),\n",
        "    ('2024-01-22 05:36:00', '2024-01-22 06:06:00'),\n",
        "    ('2024-01-22 06:14:00', '2024-01-22 06:44:00'),#6pm\n",
        "    ('2024-01-22 07:36:00', '2024-01-22 07:45:00'),\n",
        "    ('2024-01-22 08:07:00', '2024-01-22 08:37:00'),\n",
        "    ('2024-01-23 06:07:00', '2024-01-23 06:37:00'),\n",
        "    ('2024-01-23 11:30:00', '2024-01-23 12:00:00'),\n",
        "    ('2024-01-23 12:15:00', '2024-01-23 12:45:00'),\n",
        "    ('2024-01-23 17:30:00', '2024-01-23 18:00:00'),\n",
        "    ('2024-01-23 10:10:00', '2024-01-23 10:40:00')\n",
        "]\n",
        "output_file = 'output.csv'\n",
        "\n",
        "extract_device_data(device, time_ranges, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWojG6YPhMNT",
        "outputId": "1ef72ec9-c8a8-4180-ea8d-1a0f83b2fe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted data saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytz\n",
        "\n",
        "def extract_device_data(device_name, time_ranges, output_file):\n",
        "    # Load datasets\n",
        "    oura_df = pd.read_csv('oura.csv', parse_dates=['timestamp'])\n",
        "    mz3_df = pd.read_csv('mz3.csv', parse_dates=['Time'])\n",
        "    empatica_df = pd.read_csv('empatica.csv', parse_dates=['minute'])\n",
        "\n",
        "    # Standardize column names\n",
        "    oura_df.rename(columns={'timestamp': 'Timestamp'}, inplace=True)\n",
        "    mz3_df.rename(columns={'Time': 'Timestamp'}, inplace=True)\n",
        "    empatica_df.rename(columns={'minute': 'Timestamp'}, inplace=True)\n",
        "\n",
        "    # Retain original timezone as UTC and then convert to Central Time\n",
        "    for df in [oura_df, mz3_df, empatica_df]:\n",
        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], utc=True)\n",
        "        df['Timestamp'] = df['Timestamp'].dt.tz_convert('US/Central')\n",
        "\n",
        "    # Set Timestamp as index\n",
        "    oura_df.set_index('Timestamp', inplace=True)\n",
        "    mz3_df.set_index('Timestamp', inplace=True)\n",
        "    empatica_df.set_index('Timestamp', inplace=True)\n",
        "\n",
        "    # Dictionary mapping device names to their corresponding data and column name\n",
        "    devices = {\n",
        "        'oura': (oura_df, 'bpm'),\n",
        "        'mz3': (mz3_df, 'hr'),\n",
        "        'empatica': (empatica_df, 'entry_count')\n",
        "    }\n",
        "\n",
        "    if device_name not in devices:\n",
        "        print(\"Invalid device name. Choose from: oura, mz3, empatica\")\n",
        "        return\n",
        "\n",
        "    df, col_name = devices[device_name]\n",
        "\n",
        "    # Resample and compute statistics\n",
        "    df_resampled = df.resample('3min').agg({col_name: ['mean', 'max', 'min', 'std', 'count']})\n",
        "    df_resampled.columns = ['mean', 'max', 'min', 'std', 'count']\n",
        "\n",
        "    # Collect filtered data for multiple time ranges\n",
        "    results = []\n",
        "    for start_time, end_time in time_ranges:\n",
        "        filtered_data = df_resampled.loc[start_time:end_time].reset_index()\n",
        "        filtered_data['Date'] = filtered_data['Timestamp'].dt.strftime('%m/%d/%Y')\n",
        "        filtered_data['Time'] = filtered_data['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "        results.append(filtered_data)\n",
        "\n",
        "    # Concatenate all results\n",
        "    final_data = pd.concat(results, ignore_index=True)\n",
        "    final_data = final_data[['Date', 'Time', 'mean', 'max', 'min', 'std', 'count']]\n",
        "\n",
        "    # Save to CSV\n",
        "    final_data.to_csv(output_file, index=False)\n",
        "    print(f\"Extracted data saved to {output_file}\")\n",
        "\n",
        "    device = 'oura'  # Change to 'mz3' or 'empatica' as needed\n",
        "    time_ranges = [\n",
        "\n",
        "      ('2024-01-22 04:00:00', '2024-01-22 04:30:00'),\n",
        "      ('2024-01-22 05:36:00', '2024-01-22 06:06:00'),\n",
        "      ('20 24-01-22 06:14:00', '2024-01-22 06:44:00'),\n",
        "      ('2024-01-22 07:36:00', '2024-01-22 07:45:00'),\n",
        "      ('2024-01-22 08:07:00', '2024-01-22 08:37:00'),\n",
        "      ('2024-01-23 06:07:00', '2024-01-23 06:37:00'),\n",
        "      ('2024-01-23 11:30:00', '2024-01-23 12:00:00'),\n",
        "      ('2024-01-23 12:15:00', '2024-01-23 12:45:00'),\n",
        "      ('2024-01-23 17:30:00', '2024-01-23 18:00:00'),\n",
        "      ('2024-01-23 10:10:00', '2024-01-23 10:40:00')\n",
        "    ]\n",
        "output_file = 'output.csv'\n",
        "\n",
        "extract_device_data(device, time_ranges, output_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "_qIob5LHdEU6",
        "outputId": "f8f723c1-e195-4dae-81a6-9a9f6beabf9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c60ff6c51801>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'output.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mextract_device_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_ranges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extract_device_data(device_name, start_time, end_time, output_file):\n",
        "    # Load datasets (files that we have)\n",
        "    oura_df = pd.read_csv('oura.csv', parse_dates=['timestamp'])\n",
        "    mz3_df = pd.read_csv('mz3.csv', parse_dates=['Time'])\n",
        "    empatica_df = pd.read_csv('empatica.csv', parse_dates=['minute'])\n",
        "\n",
        "    # Standardize column names so that when we Filter data, it is easier to call columns 'timestamp'\n",
        "    oura_df.rename(columns={'timestamp': 'Timestamp'}, inplace=True)\n",
        "    mz3_df.rename(columns={'Time': 'Timestamp'}, inplace=True)\n",
        "    empatica_df.rename(columns={'minute': 'Timestamp'}, inplace=True)\n",
        "\n",
        "    # Remove timezone info if present\n",
        "    for df in [oura_df, mz3_df, empatica_df]:\n",
        "        df['Timestamp'] = df['Timestamp'].dt.tz_localize(None)\n",
        "\n",
        "    # Set Timestamp as index\n",
        "    oura_df.set_index('Timestamp', inplace=True)\n",
        "    mz3_df.set_index('Timestamp', inplace=True)\n",
        "    empatica_df.set_index('Timestamp', inplace=True)\n",
        "\n",
        "    # Dictionary mapping device names to their corresponding data and column name\n",
        "    devices = {\n",
        "        'oura': (oura_df, 'bpm'),\n",
        "        'mz3': (mz3_df, 'hr'),\n",
        "        'empatica': (empatica_df, 'entry_count')\n",
        "    }\n",
        "\n",
        "    if device_name not in devices:\n",
        "        print(\"Invalid device name. Choose from: oura, mz3, empatica\")\n",
        "        return\n",
        "\n",
        "    df, col_name = devices[device_name]\n",
        "\n",
        "    # Resample and compute statistics\n",
        "    df_resampled = df.resample('3min').agg({col_name: ['mean', 'max', 'min', 'std']})\n",
        "    df_resampled.columns = ['mean', 'max', 'min', 'std']\n",
        "\n",
        "    # Filter by the specified time range\n",
        "    filtered_data = df_resampled.loc[start_time:end_time].reset_index()\n",
        "    filtered_data['date'] = filtered_data['Timestamp'].dt.date\n",
        "\n",
        "    # Save to CSV\n",
        "    filtered_data.to_csv(output_file, index=False)\n",
        "    print(f\"Extracted data saved to {output_file}\")\n",
        "\n",
        "# Example usage in Google Colab:\n",
        "device = 'oura'  # Change to 'mz3' or 'empatica' as needed\n",
        "start_time = '2024-01-22 05:36:00'\n",
        "end_time = '2024-01-22 06:06:00'\n",
        "output_file = 'output.csv'\n",
        "\n",
        "extract_device_data(device, start_time, end_time, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYYi3bHQiVJ8",
        "outputId": "d319237b-aa6b-4b66-e805-1455c13375c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted data saved to output.csv\n"
          ]
        }
      ]
    }
  ]
}